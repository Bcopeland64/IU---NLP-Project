{"cells":[{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["import os\n","import torch\n","from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n","from transformers import TrainingArguments, Trainer\n","from sklearn.model_selection import train_test_split\n","import gradio as gr\n","from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def load_data(folder):\n","    texts = []\n","    labels = []\n","    for filename in os.listdir(folder):\n","        if filename.endswith(\".txt\"):\n","            with open(os.path.join(folder, filename), 'r') as file:\n","                texts.append(file.read())\n","                labels.append(1 if 'pos' in filename else 0)  # 1 for positive, 0 for negative\n","    return texts, labels\n","    \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Load data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_texts, train_labels = load_data('/home/brandon/IU International University/NLP: Project/aclImdb/train')\n","test_texts, test_labels = load_data('/home/brandon/IU International University/NLP: Project/aclImdb/test')"]},{"cell_type":"markdown","metadata":{},"source":["Split the training data for validation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"]},{"cell_type":"markdown","metadata":{},"source":["Initialize the tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"]},{"cell_type":"markdown","metadata":{},"source":["Tokenize the data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n","test_encodings = tokenizer(test_texts, truncation=True, padding=True)"]},{"cell_type":"markdown","metadata":{},"source":["Convert the data to PyTorch datasets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_dataset = CustomDataset(train_encodings, train_labels)\n","val_dataset = CustomDataset(val_encodings, val_labels)\n","test_dataset = CustomDataset(test_encodings, test_labels)"]},{"cell_type":"markdown","metadata":{},"source":["Initialize the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"]},{"cell_type":"markdown","metadata":{},"source":["Training settings"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=100,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=10,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Create the Trainer and train"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer = Trainer(\n","    model=model,                         # the instantiated Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset             # evaluation dataset\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/brandon/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 2\n","  Num Epochs = 50\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 50\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["COMET INFO: No Comet API Key was found, creating an OfflineExperiment. Set up your API Key to get the full Comet experience https://www.comet.com/docs/python-sdk/advanced/#python-configuration\n","COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: keras, tensorboard, tensorflow, torch. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n","COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n","COMET INFO: Using '/home/brandon/IU International University/NLP: Project/.cometml-runs' path as offline directory. Pass 'offline_directory' parameter into constructor or set the 'COMET_OFFLINE_DIRECTORY' environment variable to manually choose where to store offline experiment archives.\n","COMET ERROR: Failed to extract scalar from SummaryWriter.add_hparams()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98dd1d50e07d4b4f856f54ebbe6885e5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["OMP: Warning #191: Forking a process while a parallel region is active is potentially unsafe.\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","{'loss': 0.6925, 'learning_rate': 1.0000000000000002e-06, 'epoch': 10.0}\n"]},{"name":"stderr","output_type":"stream","text":["OMP: Warning #191: Forking a process while a parallel region is active is potentially unsafe.\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["OMP: Warning #191: Forking a process while a parallel region is active is potentially unsafe.\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","{'loss': 0.6847, 'learning_rate': 2.0000000000000003e-06, 'epoch': 20.0}\n","{'loss': 0.7048, 'learning_rate': 3e-06, 'epoch': 30.0}\n","{'loss': 0.6891, 'learning_rate': 4.000000000000001e-06, 'epoch': 40.0}\n"]},{"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.6719, 'learning_rate': 5e-06, 'epoch': 50.0}\n","{'train_runtime': 417.5416, 'train_samples_per_second': 0.239, 'train_steps_per_second': 0.12, 'train_loss': 0.6886120796203613, 'epoch': 50.0}\n"]},{"data":{"text/plain":["TrainOutput(global_step=50, training_loss=0.6886120796203613, metrics={'train_runtime': 417.5416, 'train_samples_per_second': 0.239, 'train_steps_per_second': 0.12, 'train_loss': 0.6886120796203613, 'epoch': 50.0})"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["Evaluate the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 64\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"69f0c940ce5d40079bac681673e7297e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 0.6953861713409424,\n"," 'eval_runtime': 2.0121,\n"," 'eval_samples_per_second': 0.994,\n"," 'eval_steps_per_second': 0.497,\n"," 'epoch': 50.0}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict_review(text):\n","    # Tokenize and encode the input text\n","    encodings = tokenizer(text, truncation=True, padding=True, return_tensors='pt')\n","    \n","    # Get the model's output\n","    output = model(**encodings)\n","    \n","    # Convert output to probabilities\n","    probabilities = torch.nn.functional.softmax(output.logits, dim=-1)\n","    \n","    # Get the predicted class\n","    predicted_class = torch.argmax(probabilities)\n","    \n","    return 'positive' if predicted_class.item() == 1 else 'negative'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["iface = gr.Interface(fn=predict_review,\n","                     inputs=gr.Textbox(lines=2, placeholder=\"Enter comment here...\"),\n","                     outputs=\"text\",\n","                     allow_flagging='never')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on local URL:  http://127.0.0.1:7860\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Running on public URL: https://82914d2becc8c8781b.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"]},{"data":{"text/html":["<div><iframe src=\"https://82914d2becc8c8781b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":[]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["iface.launch(share=True)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":2}
